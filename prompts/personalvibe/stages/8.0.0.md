## Milestone Analysis: Sandboxing and Automated Testing Framework

The personalvibe project has evolved into a sophisticated developer tool for vibecoding, providing an effective workflow from brainstorming to code generation through structured milestones and sprints. The current codebase demonstrates mature patterns with comprehensive CLI tooling, robust configuration management, and multi-provider LLM integration. However, the manual intervention required after code generation represents a significant friction point that undermines the tool's potential for automated development workflows.

This milestone addresses the critical gap between code generation and validation by implementing Docker-based sandboxing and automated testing. The approach leverages the existing retry engine architecture while introducing containerized execution environments that protect the native codebase from potentially destructive patches. This enhancement transforms personalvibe from a code generation tool into a complete automated development pipeline, enabling developers to confidently iterate on generated solutions without manual oversight of each execution step.

Based on the current codebase analysis (~2,100 lines across 20+ modules) and the sandboxing requirements, I estimate the total implementation will require approximately 12,000-14,000 characters across infrastructure, core logic, and integration components. The work naturally segments into discrete, testable chunks that build upon existing architectural patterns.

## Implementation Chunks

**Chunk 1: Docker Environment Manager (Priority 1)**
*Estimated size: ~2,800 characters*
Create the core Docker orchestration module (`src/personalvibe/docker_manager.py`) that handles container lifecycle, volume mounting, and environment isolation. This foundational component must integrate with the existing `vibe_utils.get_base_path()` pattern and provide clean interfaces for mounting project directories as read-only with separate writable workspaces. Include Docker availability detection and graceful fallback error handling to satisfy the optional Docker requirement.

**Chunk 2: Sandboxed Execution Engine (Priority 2)**
*Estimated size: ~3,200 characters*
Implement the sandbox execution logic that coordinates between Docker containers and the existing patch application workflow. This extends the current `parse_stage.py` functionality with containerized patch execution, log capture, and result validation. The engine must handle the five-step process: patch generation, sandbox loading, execution, error capture, and test suite running, while integrating with the existing retry engine architecture.

**Chunk 3: Test Suite Integration (Priority 3)**
*Estimated size: ~2,400 characters*
Develop the test configuration and execution framework that enables users to specify test commands through YAML configuration. This chunk extends the `ConfigModel` schema to include test suite paths and commands, then implements the test execution logic within sandboxed environments. Include support for both shell scripts and Python test frameworks, with comprehensive log capture and result interpretation.

**Chunk 4: Enhanced Configuration and CLI Integration (Priority 4)**
*Estimated size: ~2,600 characters*
Update the YAML configuration schema and CLI interface to support the new sandboxing parameters. This includes adding Docker-specific configuration options, test suite specifications, and sandbox behavior controls. Modify the `run_pipeline.py` and CLI modules to seamlessly integrate sandboxed execution with existing sprint workflows, maintaining backward compatibility for users who don't require sandboxing.

**Chunk 5: Retry Logic and Error Handling Enhancement (Priority 5)**
*Estimated size: ~2,000 characters*
Enhance the existing `retry_engine.py` to work with sandboxed environments, implementing the five-retry logic with proper cleanup of Docker containers and volumes. This chunk focuses on robust error handling, log aggregation across multiple retry attempts, and integration with the existing git branch management for failed sprints. Include comprehensive logging to help users understand why retries occurred and what corrective actions were attempted.
