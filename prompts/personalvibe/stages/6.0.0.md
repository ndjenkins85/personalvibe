Below is the high-level plan that guides Personalvibe from today’s “direct-OpenAI” implementation to a flexible LiteLLM-powered stack.

I first explain the reasoning principles that shaped the plan, then lay out the milestone and its chunked execution order.

Introductory reasoning
The codebase is stable, well-tested, and already abstracts most runtime concerns (prompt persistence, logging, workspace root).  However, the hard dependency on the openai-python SDK keeps us locked to a single provider, complicates cost/routing control, and prevents experiments with local or custom models.  LiteLLM solves those problems in one stroke and is mature enough to justify the migration effort.
Because the repository has ~25 000 + lines/200 K+ characters of Python and docs, we must avoid a “big-bang” patch that would overflow both LLM output limits (≈20 000 characters) and human review capacity.  The milestone therefore slices work into ≤5 coherent, testable chunks that build new scaffolding first, migrate call-sites next, and only then add custom providers & docs.

Milestone: “LiteLLM Integration & Model Pluggability”
Goal – The codebase must route **all** LLM traffic through LiteLLM, defaulting to `openai/gpt-4o-mini` (alias `o3`) yet allowing any `<provider>/<model>` string (including the custom `my-custom-llm/sharp_gemma3_12b_128k`).  End-users select the model via a new optional `model:` field in every YAML config.  Existing behaviour stays unchanged when the field is absent.  CI must pass with no OpenAI network calls (monkey-patched fake).

Estimated milestone size: ≈15 000–18 000 characters of new/changed code + tests + docs.  Well within one 20 k window, but we keep headroom by shipping in smaller parts.

Work chunks (rank-ordered)

Chunk 1 – LiteLLM shim & dependency (scaffolding)
• Add `litellm>=1.40` to `pyproject.toml`
• Create `personalvibe/llm_router.py` with a single public helper `chat_completion(model:str|None, messages:list, **kw)` that internally calls `litellm.completion` (sync) and applies defaults (`model="openai/gpt-4o-mini"` if None).
• Unit-tests: happy path (mock litellm), bad model raises, streaming not yet.
Approx. size: 3 k chars.

Chunk 2 – Config schema & CLI plumbing
• Extend `ConfigModel` with optional `model: str = ""` (validate `<prov>/<name>` or empty).
• Pass `config.model` down the call-chain (`run_pipeline → vibe_utils.get_vibed → llm_router.chat_completion`).
• Tests: YAML round-trip, fallback to default when field missing.
Approx. size: 2.5 k chars.

Chunk 3 – Replace OpenAI usage in vibe_utils.get_vibed
• Drop direct openai-python import; instead build `messages` and call the new llm_router.
• Remove/gate OPENAI_API_KEY bootstrap logic; instead rely on LiteLLM env detection.
• Make `max_completion_tokens` honoured by forwarding to LiteLLM.
• Update existing tests to monkey-patch llm_router rather than openai.
Approx. size: 4 k chars.

Chunk 4 – Custom provider “sharp_boe”
• Implement the `MyCustomLLM` class exactly as in the prompt; register in `llm_router.__init__` when `SHARP_USER_SECRET` is set.
• Add integration test with responses mocked via `responses` or `httpretty`.
Approx. size: 3 k chars.

Chunk 5 – Docs, examples & deprecation clean-up
• Update README, INSTALL, prompt templates to mention `model:` knob.
• Add `docs/LLM_PROVIDERS.md` summarising supported strings.
• Mark old OPENAI-specific helper code as deprecated and schedule removal.
• Final smoke-test ensures default run passes without setting any model field.
Approx. size: 2 k chars.

Why this order
Chunk 1 lays the foundation others depend on; without the shim nothing compiles.
Chunk 2 propagates the new configuration so later migrations can compile.
Chunk 3 switches the core execution path—riskier, but isolated by earlier scaffolding.
Chunk 4 is optional functionality that proves pluggability without blocking migration.
Chunk 5 polishes and documents, safest to do after behaviour stabilises.

This plan fits comfortably under the 20 000-character generation cap, splits concerns cleanly, and lets both CI and human reviewers verify success incrementally.
